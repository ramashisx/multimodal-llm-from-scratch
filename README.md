# Multimodal LLM from Scratch

This repository contains code and resources for building a multimodal large language model (LLM) from scratch. This is a code along implementation of tutorial by Umar Jamil [Video](https://www.youtube.com/watch?v=vAmKB7iPkWw)

## Introduction

Multimodal LLMs are designed to understand and generate human-like text by leveraging various data types. This project explores the techniques and methodologies required to build such models from the ground up.

## Installation

To get started, clone the repository and install the necessary dependencies:

```bash
git clone https://github.com/ramashisx/multimodal-llm-from-scratch.git
cd multimodal-llm-from-scratch
pip install -r requirements.txt
```

## Usage

Provide instructions on how to use the code, including examples and command-line options.

```bash
bash run_inference.sh
```

